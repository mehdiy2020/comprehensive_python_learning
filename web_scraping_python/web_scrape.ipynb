{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c76b63d0",
   "metadata": {},
   "source": [
    "## Comprehensive Web Scraping in Python\n",
    "Web scraping involves extracting data from websites. Here's a complete guide covering all relevant functions, libraries, and techniques.\n",
    "\n",
    "### Table of Contents\n",
    "[Essential Libraries](#Essential-Libraries)\n",
    "\n",
    "[Basic Scraping with Requests and BeautifulSoup](#Basic-Scraping-with-Requests-and-BeautifulSoup)\n",
    "\n",
    "[Dynamic Content with Selenium](#Dynamic-Content-with-Selenium)\n",
    "\n",
    "[API Scraping](#API-Scraping)\n",
    "\n",
    "[Handling Common Challenges](#Handling-Common-Challenges)\n",
    "\n",
    "[Best Practices & Ethics](Best_Practices_&_Ethics)\n",
    "\n",
    "[Complete Examples](#Complete_Examples)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Essential Libraries\n",
    "\n",
    "### Core scraping libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium.webdriver as webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "### Data handling\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "### Utilities\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import os\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "## Basic Scraping with Requests and BeautifulSoup\n",
    "\n",
    "1. Making HTTP Requests\n",
    "```\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def simple_get_request(url):\n",
    "    \"\"\"Basic GET request with error handling\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises HTTPError for bad responses\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "```\n",
    "\n",
    "# Example usage\n",
    "`html_content = simple_get_request(\"https://httpbin.org/html\")`\n",
    "\n",
    "2. Advanced Request Configuration\n",
    "\n",
    "```def advanced_get_request(url, headers=None, params=None, timeout=10):\n",
    "    \"\"\"Advanced GET request with headers and parameters\"\"\"\n",
    "    \n",
    "    # Default headers\n",
    "    default_headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "    }\n",
    "    \n",
    "    if headers:\n",
    "        default_headers.update(headers)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url,\n",
    "            headers=default_headers,\n",
    "            params=params,\n",
    "            timeout=timeout\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "```\n",
    "\n",
    "# Example with parameters and headers\n",
    "```\n",
    "url = \"https://httpbin.org/get\"\n",
    "params = {'key1': 'value1', 'key2': 'value2'}\n",
    "headers = {'User-Agent': 'My Custom Agent 1.0'}\n",
    "\n",
    "response = advanced_get_request(url, headers=headers, params=params)\n",
    "if response:\n",
    "    print(response.json())\n",
    "```\n",
    "\n",
    "3. BeautifulSoup Parsing\n",
    "\n",
    "```\n",
    "def parse_html(html_content, parser='html.parser'):\n",
    "    \"\"\"Parse HTML content with BeautifulSoup\"\"\"\n",
    "    soup = BeautifulSoup(html_content, parser)\n",
    "    return soup\n",
    "\n",
    "def extract_data(soup):\n",
    "    \"\"\"Extract various types of data from parsed HTML\"\"\"\n",
    "    \n",
    "    # Extract title\n",
    "    title = soup.title.string if soup.title else \"No title\"\n",
    "    \n",
    "    # Extract all links\n",
    "    links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        links.append({\n",
    "            'text': link.get_text(strip=True),\n",
    "            'url': link['href']\n",
    "        })\n",
    "    \n",
    "    # Extract all paragraphs\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
    "    \n",
    "    # Extract by CSS class\n",
    "    specific_elements = [elem.get_text(strip=True) \n",
    "                        for elem in soup.find_all(class_='specific-class')]\n",
    "    \n",
    "    # Extract by ID\n",
    "    main_content = soup.find(id='main-content')\n",
    "    main_text = main_content.get_text(strip=True) if main_content else \"\"\n",
    "    \n",
    "    return {\n",
    "        'title': title,\n",
    "        'links': links,\n",
    "        'paragraphs': paragraphs,\n",
    "        'specific_elements': specific_elements,\n",
    "        'main_content': main_text\n",
    "    }\n",
    "```\n",
    "\n",
    "# Complete example\n",
    "```\n",
    "def scrape_website(url):\n",
    "    \"\"\"Complete scraping function\"\"\"\n",
    "    html = simple_get_request(url)\n",
    "    if html:\n",
    "        soup = parse_html(html)\n",
    "        data = extract_data(soup)\n",
    "        return data\n",
    "    return None\n",
    "```\n",
    "\n",
    "# Usage\n",
    "```\n",
    "data = scrape_website(\"https://httpbin.org/html\")\n",
    "print(data)\n",
    "```\n",
    "4. Finding Elements with BeautifulSoup\n",
    "\n",
    "```\n",
    "def comprehensive_element_finding(soup):\n",
    "    \"\"\"Demonstrate various ways to find elements\"\"\"\n",
    "    \n",
    "    # Find by tag name\n",
    "    all_links = soup.find_all('a')\n",
    "    all_images = soup.find_all('img')\n",
    "    \n",
    "    # Find by class\n",
    "    news_items = soup.find_all('div', class_='news-item')\n",
    "    featured = soup.find_all(class_='featured')\n",
    "    \n",
    "    # Find by ID\n",
    "    header = soup.find(id='header')\n",
    "    footer = soup.find(id='footer')\n",
    "    \n",
    "    # Find by attributes\n",
    "    images_with_alt = soup.find_all('img', alt=True)\n",
    "    links_with_target = soup.find_all('a', target='_blank')\n",
    "    \n",
    "    # Find by text content\n",
    "    contact_links = soup.find_all('a', string=re.compile('contact', re.I))\n",
    "    price_elements = soup.find_all(string=re.compile(r'\\$\\d+'))\n",
    "    \n",
    "    # CSS selectors\n",
    "    navigation_links = soup.select('nav a')\n",
    "    first_paragraph = soup.select_one('p:first-of-type')\n",
    "    specific_divs = soup.select('div.container > div.row > div.col')\n",
    "    \n",
    "    # Find parent, children, siblings\n",
    "    if header:\n",
    "        parent = header.find_parent()\n",
    "        children = header.find_all(recursive=False)\n",
    "        next_sibling = header.find_next_sibling()\n",
    "        previous_sibling = header.find_previous_sibling()\n",
    "    \n",
    "    return {\n",
    "        'all_links': [link.get('href') for link in all_links],\n",
    "        'images_with_alt': [img.get('alt') for img in images_with_alt],\n",
    "        'contact_links': [link.get_text() for link in contact_links],\n",
    "        'navigation_links': [link.get('href') for link in navigation_links]\n",
    "    }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141538ea",
   "metadata": {},
   "source": [
    "## Basics & Review\n",
    "\n",
    "`#relevant_tag` use to select  or address id - id attributes are normally unique on a webpage\n",
    "\n",
    "`.relevant_class` use to select or address classes - class atribute are not uniue for a webpage\n",
    "\n",
    "### What Beautfulsoup does:\n",
    "\n",
    "```\n",
    "from bs4 import beautifulsoup\n",
    "\n",
    "# this can parse the html\n",
    "\n",
    "outcome_of_parsing = Beautifulsoup(out_html, the_partsing(here): 'html.parser')\n",
    "# Stantiating New Object\n",
    "\n",
    "\n",
    "# with Tag\n",
    "print(outcome_of_parsing.find('h1')) --> <h1>sssssssssssanythingsssssss.</h1>\n",
    "\n",
    "# without Tag\n",
    "print(outcome_of_parsing.find('h1').string) --> 'sssssssssssanythingsssssss.'\n",
    "\n",
    "def find_title():\n",
    "  h1_tag = outcome_of_parsing.find('h1')\n",
    "  print(h1_tag.string)\n",
    "\n",
    "def find_list_items():\n",
    "  list_items = outcome_of_parsing.find_all('li')\n",
    "  print(list_items) # It is a List\n",
    "\n",
    "or even better\n",
    "\n",
    "def find_list_items():\n",
    "  list_items = outcome_of_parsing.find_all('li')\n",
    "  list_conteent = [e.string for e in list_items]\n",
    "  print(list_content)\n",
    "\n",
    "<p class = \"subtitle\">ssssssssss.sssssssss</p>\n",
    "def find_para():\n",
    "  para = outcome_of_parsing.find('p', attrs:{'class': 'subtitle})\n",
    "  print(para.string)\n",
    "\n",
    "<p class = \"subtitle\">ssssssssss.sssssssss</p>\n",
    "<p>dddddddddddddddddddddd</p>\n",
    "def find_paras():\n",
    "  paras = outcome_of_parsing.find_all('p')\n",
    "  other_para = [par for par in paras if 'subtitle' not in p.attrs.get('class', []) ]\n",
    "  print(pther_para[0].string)\n",
    "```\n",
    "\n",
    "#### Now we need to exctract from tags as well\n",
    "\n",
    "```\n",
    "<article product_pod>\n",
    "....\n",
    "<h3><a href='ddddd', title='vvvvvv'>....\n",
    "...\n",
    "</article>\n",
    "\n",
    "\n",
    "def get_book_title():\n",
    "  locator = 'article h3 a' or 'article.product_pod h3 a' # CSS Locator\n",
    "  ## Below we get the first matching element from the parsed HTML\n",
    "  item_link = outcome_of_parsing.select_one(locator) # This Function gives CSS Selector (tag, class, id, etc.)\n",
    "  ## The item_link.attrs return a dictionary of all the class, id,... within the \"a\" Tag here\n",
    "  item_name = item_link.attrs['title']\n",
    "\n",
    "def get_book_link():\n",
    "  locator = 'article h3 a' or 'article.product_pod h3 a' # CSS Locator\n",
    "  ## Below we get the first matching element from the parsed HTML\n",
    "  item_link = outcome_of_parsing.select_one(locator) \n",
    "  ## The item_link.attrs return a dictionary of all the class, id,... within the \"a\" Tag here\n",
    "  item_name = item_link.attrs['href']\n",
    "\n",
    "\n",
    "def item_price():\n",
    "  locator = 'article.product_pod p.price_color'\n",
    "  the_price = outcome_of_parsing.select_one(locator)\n",
    "  price_out = the_pice.text().replace(\"$\", \"\")\n",
    "  float(price_out)\n",
    "```\n",
    "#### Note: \n",
    "\n",
    "### ✅ tag.text\n",
    "\n",
    "Returns all the text content inside the tag, including nested tags.\n",
    "\n",
    "Always returns a string, even if the tag contains multiple children or complex HTML.\n",
    "\n",
    "### ⚠️ tag.string\n",
    "\n",
    "Returns the string only if the tag contains a single NavigableString (i.e., just plain text).\n",
    "\n",
    "If the tag contains nested tags or multiple text nodes, it returns None.\n",
    "\n",
    "```\n",
    "tag = soup.select_one(\".note\")\n",
    "print(tag.string)  # Output: Hello\n",
    "```\n",
    "\n",
    "`<div class=\"note\">Hello <b>World</b></div>\n",
    "`\n",
    "```\n",
    "tag = soup.select_one(\".note\")\n",
    "print(tag.string)  # Output: None\n",
    "print(tag.text)    # Output: Hello World\n",
    "```\n",
    "\n",
    "def book_rate():\n",
    "  locator = 'article.product_pot p.star_rating'\n",
    "  item_rate = outcome_of_parsing.select_one(locator) \n",
    "  classes_outcome = item_rate.attrs['class']\n",
    "  list_of_rate = [rt for rt in classes_coutcome of re != \"star-rating\"]\n",
    "  or\n",
    "  list_of_rate = filter(lambda x: X != 'star-rate', classes_outcome)\n",
    "  print(lis_of_rate[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83360e86",
   "metadata": {},
   "source": [
    "## Structure your files \n",
    "\n",
    "1. if you have the page you have file for specifics to extract from the page\n",
    "Create class of ParseItem and change the print to return \n",
    "note\n",
    " def __init__(self, page (or portion of a page)):\n",
    "  self.soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "remember add @property to avoid brackets for methods that you create in this file\n",
    "\n",
    "\n",
    "2. Separate locators from the class item (and in a different file)\n",
    "\n",
    "\n",
    "class ParsedItemLocator:\n",
    "\n",
    "  NAME_LOCATOR = \"article.product_pod h3 a\"\n",
    "  LINK_LOCATOR = \"article.product_pod h3 a\"\n",
    "  PRICE_LOCATOR = \"article.product_pod p.price_color\"\n",
    "  RATING_LOCATOR = \"article.product_pod p.star-rating\"\n",
    "\n",
    "\n",
    "\n",
    "after importing locator for any of method (previous functions) become locator = ParsedItemLocators.NAME_LOCATOR, and so on, and so forth...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a165a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1afa2768",
   "metadata": {},
   "source": [
    "## Dynamic Content with Selenium\n",
    "\n",
    "1. Basic Selenium Setup\n",
    "\n",
    "```\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def setup_driver(headless=True):\n",
    "    \"\"\"Setup Chrome driver with options\"\"\"\n",
    "    chrome_options = Options()\n",
    "    \n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "    \n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    \n",
    "    # Prevent detection\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    return driver\n",
    "```\n",
    "\n",
    "```\n",
    "def smart_wait(driver, selector, by=By.CSS_SELECTOR, timeout=10):\n",
    "    \"\"\"Wait for element to be present and visible\"\"\"\n",
    "    try:\n",
    "        element = WebDriverWait(driver, timeout).until(\n",
    "            EC.presence_of_element_located((by, selector))\n",
    "        )\n",
    "        return element\n",
    "    except Exception as e:\n",
    "        print(f\"Element not found: {selector}\")\n",
    "        return None\n",
    "```\n",
    "\n",
    "2. Dynamic Content Scraping\n",
    "\n",
    "\n",
    "```\n",
    "def scrape_dynamic_content(url, wait_for_selector=None):\n",
    "    \"\"\"Scrape content from JavaScript-heavy websites\"\"\"\n",
    "    driver = setup_driver(headless=True)\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for specific element if provided\n",
    "        if wait_for_selector:\n",
    "            smart_wait(driver, wait_for_selector)\n",
    "        \n",
    "        # Wait for page to load completely\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Get page source after JavaScript execution\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "        # Extract data\n",
    "        data = extract_data(soup)\n",
    "        \n",
    "        # Take screenshot for debugging\n",
    "        driver.save_screenshot('page_screenshot.png')\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        driver.quit()\n",
    "```\n",
    "# Example: Scrolling and loading more content\n",
    "\n",
    "```\n",
    "def infinite_scroll_scrape(url, scroll_pause_time=2, max_scrolls=10):\n",
    "    \"\"\"Handle infinite scroll pages\"\"\"\n",
    "    driver = setup_driver(headless=False)\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        \n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        scrolls = 0\n",
    "        \n",
    "        while scrolls < max_scrolls:\n",
    "            # Scroll to bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "            \n",
    "            # Calculate new scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            \n",
    "            if new_height == last_height:\n",
    "                break\n",
    "                \n",
    "            last_height = new_height\n",
    "            scrolls += 1\n",
    "        \n",
    "        # Now scrape the fully loaded content\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        return extract_data(soup)\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed6cb98",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b41430a",
   "metadata": {},
   "source": [
    "## API Scraping\n",
    "\n",
    "1. REST API Scraping\n",
    "\n",
    "```   \n",
    "def scrape_api(endpoint, headers=None, params=None, method='GET'):\n",
    "    \"\"\"Scrape data from REST APIs\"\"\"\n",
    "    \n",
    "    default_headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json',\n",
    "    }\n",
    "    \n",
    "    if headers:\n",
    "        default_headers.update(headers)\n",
    "    \n",
    "    try:\n",
    "        if method.upper() == 'GET':\n",
    "            response = requests.get(endpoint, headers=default_headers, params=params)\n",
    "        elif method.upper() == 'POST':\n",
    "            response = requests.post(endpoint, headers=default_headers, json=params)\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Handle different response formats\n",
    "        content_type = response.headers.get('content-type', '')\n",
    "        \n",
    "        if 'application/json' in content_type:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return response.text\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API Error: {e}\")\n",
    "        return None\n",
    "```\n",
    "\n",
    "# Example: Paginated API \n",
    "\n",
    "```\n",
    "def scrape_paginated_api(base_url, page_param='page', max_pages=100):\n",
    "    \"\"\"Scrape paginated API endpoints\"\"\"\n",
    "    all_data = []\n",
    "    page = 1\n",
    "    \n",
    "    while page <= max_pages:\n",
    "        params = {page_param: page}\n",
    "        data = scrape_api(base_url, params=params)\n",
    "        \n",
    "        if not data or len(data) == 0:\n",
    "            break\n",
    "            \n",
    "        all_data.extend(data)\n",
    "        page += 1\n",
    "        \n",
    "        # Be respectful\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return all_data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3025d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0d8cb1b",
   "metadata": {},
   "source": [
    "## Handling Common Challenges\n",
    "\n",
    "1. Rate Limiting and Delays\n",
    "\n",
    "```\n",
    "import random\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def respectful_scraping(delay_range=(1, 3)):\n",
    "    \"\"\"Decorator to add random delays between requests\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Random delay\n",
    "            delay = random.uniform(delay_range[0], delay_range[1])\n",
    "            time.sleep(delay)\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "class RateLimitedScraper:\n",
    "    \"\"\"Rate limiting class for multiple requests\"\"\"\n",
    "    \n",
    "    def __init__(self, requests_per_minute=60):\n",
    "        self.requests_per_minute = requests_per_minute\n",
    "        self.delay = 60.0 / requests_per_minute\n",
    "        self.last_request_time = 0\n",
    "    \n",
    "    def wait(self):\n",
    "        \"\"\"Wait if necessary to respect rate limit\"\"\"\n",
    "        elapsed = time.time() - self.last_request_time\n",
    "        if elapsed < self.delay:\n",
    "            time.sleep(self.delay - elapsed)\n",
    "        self.last_request_time = time.time()\n",
    "    \n",
    "    @respectful_scraping(delay_range=(1, 2))\n",
    "    def scrape(self, url):\n",
    "        \"\"\"Scrape with rate limiting\"\"\"\n",
    "        self.wait()\n",
    "        return simple_get_request(url)\n",
    "```\n",
    "\n",
    "2. Proxy Rotation\n",
    "\n",
    "```\n",
    "class ProxyManager:\n",
    "    \"\"\"Manage proxy rotation for scraping\"\"\"\n",
    "    \n",
    "    def __init__(self, proxy_list=None):\n",
    "        self.proxies = proxy_list or []\n",
    "        self.current_proxy_index = 0\n",
    "    \n",
    "    def get_proxy(self):\n",
    "        \"\"\"Get next proxy in rotation\"\"\"\n",
    "        if not self.proxies:\n",
    "            return None\n",
    "        \n",
    "        proxy = self.proxies[self.current_proxy_index]\n",
    "        self.current_proxy_index = (self.current_proxy_index + 1) % len(self.proxies)\n",
    "        return proxy\n",
    "    \n",
    "    def scrape_with_proxy(self, url):\n",
    "        \"\"\"Scrape using current proxy\"\"\"\n",
    "        proxy = self.get_proxy()\n",
    "        \n",
    "        if proxy:\n",
    "            try:\n",
    "                response = requests.get(url, proxies={'http': proxy, 'https': proxy}, timeout=10)\n",
    "                return response.text\n",
    "            except:\n",
    "                print(f\"Proxy failed: {proxy}\")\n",
    "                return None\n",
    "        else:\n",
    "            return simple_get_request(url)\n",
    "```\n",
    "# Example proxy list\n",
    "proxies = [\n",
    "    'http://proxy1:port',\n",
    "    'http://proxy2:port', \n",
    "    'http://proxy3:port'\n",
    "]\n",
    "\n",
    "proxy_manager = ProxyManager(proxies)\n",
    "\n",
    "3. Handling Sessions and Cookies\n",
    "\n",
    "\n",
    "```\n",
    "def session_based_scraping(login_url, credentials, target_url):\n",
    "    \"\"\"Maintain session across multiple requests\"\"\"\n",
    "    \n",
    "    with requests.Session() as session:\n",
    "        # Login\n",
    "        login_data = {\n",
    "            'username': credentials['username'],\n",
    "            'password': credentials['password']\n",
    "        }\n",
    "        \n",
    "        # Get login page first (for CSRF tokens, etc.)\n",
    "        login_page = session.get(login_url)\n",
    "        soup = BeautifulSoup(login_page.text, 'html.parser')\n",
    "        \n",
    "        # Extract CSRF token if present\n",
    "        csrf_token = soup.find('input', {'name': 'csrf_token'})\n",
    "        if csrf_token:\n",
    "            login_data['csrf_token'] = csrf_token['value']\n",
    "        \n",
    "        # Perform login\n",
    "        login_response = session.post(login_url, data=login_data)\n",
    "        login_response.raise_for_status()\n",
    "        \n",
    "        # Now access protected content\n",
    "        target_response = session.get(target_url)\n",
    "        return target_response.text\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca25279",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f27cfd4",
   "metadata": {},
   "source": [
    "## Data Storage Functions\n",
    "\n",
    "1. Save to CSV\n",
    "\n",
    "```\n",
    "def save_to_csv(data, filename):\n",
    "    \"\"\"Save scraped data to CSV\"\"\"\n",
    "    if not data:\n",
    "        print(\"No data to save\")\n",
    "        return\n",
    "    \n",
    "    # Determine fieldnames\n",
    "    if isinstance(data, list) and len(data) > 0:\n",
    "        fieldnames = data[0].keys()\n",
    "    else:\n",
    "        fieldnames = data.keys()\n",
    "        data = [data]\n",
    "    \n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "    \n",
    "    print(f\"Data saved to {filename}\")\n",
    "```\n",
    "2. Save to JSON\n",
    "\n",
    "```\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"Save scraped data to JSON\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as jsonfile:\n",
    "        json.dump(data, jsonfile, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Data saved to {filename}\")\n",
    "```\n",
    "\n",
    "\n",
    "3. Save to Database\n",
    "\n",
    "```\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def save_to_sqlite(data, db_file, table_name):\n",
    "    \"\"\"Save data to SQLite database\"\"\"\n",
    "    if isinstance(data, list):\n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        df = pd.DataFrame([data])\n",
    "    \n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Data saved to {db_file}.{table_name}\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5966d1f8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
